{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# define image size\n",
    "img_rows=28\n",
    "img_cols=28\n",
    "# load dataset\n",
    "(X_train , y_train) ,(X_test , y_test ) = mnist.load_data()\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "\n",
    "# reshape\n",
    "X_train= X_train.reshape(X_train.shape [0], img_rows , img_cols , 1)\n",
    "X_test= X_test.reshape(X_test.shape [0], img_rows , img_cols , 1)\n",
    "# one hot encoded\n",
    "y_train= np_utils.to_categorical(y_train )\n",
    "y_test= np_utils.to_categorical(y_test)\n",
    "# fix random seed for reproducibility\n",
    "seed = 100\n",
    "np.random.seed(seed)\n",
    "num_classes= 10\n",
    "\n",
    "# create CNN model\n",
    "def cnn_model():\n",
    "    # define model9\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, kernel_size =(5, 5),\n",
    "                            #border_mode=='valid', \n",
    "                            strides=(1, 1), \n",
    "                            input_shape=(img_rows , img_cols , 1), \n",
    "                            activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size =(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(127, activation='relu'))\n",
    "    model.add(Dense(num_classes , activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = cnn_model()\n",
    "# Fit the model\n",
    "disp= model.fit(X_train , y_train ,validation_data=(X_test , y_test ),nb_epoch=10,batch_size=200,verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores =model.evaluate(X_test , y_test , verbose=0)\n",
    "print(\"loss: %.2f\" % scores[0])\n",
    "print(\"acc: %.2f\" % scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.convolutional import Convolution2D\n",
    "from keras.layers.convolutional import MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "# define image size\n",
    "img_rows=28\n",
    "img_cols=28\n",
    "# load dataset\n",
    "(X_train , y_train) ,(X_test , y_test ) = mnist.load_data()\n",
    "'''print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_train)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_1 = 32\n",
    "img_2 = 32\n",
    "img_3 = 3\n",
    "\n",
    "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
    "# reshape\n",
    "X_train= X_train.reshape(X_train.shape [0], img_1 , img_2 , img_3)\n",
    "X_test= X_test.reshape(X_test.shape [0], img_1 , img_2 , img_3)\n",
    "# one hot encoded\n",
    "y_train= np_utils.to_categorical(y_train )\n",
    "y_test= np_utils.to_categorical(y_test)\n",
    "# fix random seed for reproducibility\n",
    "seed = 100\n",
    "np.random.seed(seed)\n",
    "num_classes= 10\n",
    "\n",
    "# create CNN model\n",
    "def cnn_model():\n",
    "    # define model9\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, kernel_size =(5, 5),#아마 여기가 문제\n",
    "                            #border_mode=='valid', \n",
    "                            strides=(1, 1), \n",
    "                            input_shape=(img_1 , img_2 , img_3), \n",
    "                            activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size =(2,2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(127, activation='relu'))\n",
    "    model.add(Dense(num_classes , activation='softmax'))\n",
    "    # Compile model\n",
    "    model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# build the model\n",
    "model = cnn_model()\n",
    "# Fit the model\n",
    "disp= model.fit(X_train , y_train ,validation_data=(X_test , y_test),nb_epoch=10,batch_size=200,verbose=1)\n",
    "# Final evaluation of the model\n",
    "scores =model.evaluate(X_test , y_test , verbose=0)\n",
    "print(\"loss: %.2f\" % scores[0])\n",
    "print(\"acc: %.2f\" % scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "from keras.datasets import cifar10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def im2col(input_data, filter_h, filter_w, stride=1, pad=0):\n",
    "    \"\"\"\n",
    "    다수의 이미지를 입력받아 2차원 배열로 변환한다(평탄화).\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    input_data : 4차원 배열 형태의 입력 데이터(이미지 수, 채널 수, 높이, 너비)\n",
    "    filter_h : 필터의 높이\n",
    "    filter_w : 필터의 너비\n",
    "    stride : 스트라이드\n",
    "    pad : 패딩\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    col : 2차원 배열\n",
    "    \"\"\"\n",
    "    N, C, H, W = input_data.shape\n",
    "    out_h = (H + 2 * pad - filter_h) // stride + 1\n",
    "    out_w = (W + 2 * pad - filter_w) // stride + 1\n",
    "\n",
    "    img = np.pad(input_data, [(0, 0), (0, 0), (pad, pad), (pad, pad)], 'constant')\n",
    "    col = np.zeros((N, C, filter_h, filter_w, out_h, out_w))\n",
    "\n",
    "    for y in range(filter_h):\n",
    "        y_max = y + stride * out_h\n",
    "        for x in range(filter_w):\n",
    "            x_max = x + stride * out_w\n",
    "            col[:, :, y, x, :, :] = img[:, :, y:y_max:stride, x:x_max:stride]\n",
    "\n",
    "    col = col.transpose(0, 4, 5, 1, 2, 3).reshape(N * out_h * out_w, -1)\n",
    "    return col\n",
    "\n",
    "class Convolution:\n",
    "    def __init__(self, W, b, stride=1, pad=0):\n",
    "        self.W = W\n",
    "        self.b = b\n",
    "        self.stride = stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        FN, C, FH, FW = self.W.shape\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H + 2*self.pad - FH) / self. stride)\n",
    "        out_w = int(1 + (H + 2*self.pad - FW) / self. stride)\n",
    "        \n",
    "        col = im2col(x, FH, FW, self.stride, self.pad)\n",
    "        col_W = self.W.reshape(FN, -1).T # 필터 전개\n",
    "        \n",
    "        out = np.dot(col, col_W) + self.b\n",
    "        \n",
    "        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class Pooling:\n",
    "    def __init__(self, pool_h, pool_w, stride=1, pad=0):\n",
    "        self.pool_h = pool_h\n",
    "        self.pool_w = pool_w\n",
    "        self.stride = self.stride\n",
    "        self.pad = pad\n",
    "        \n",
    "    def forward(self, x):\n",
    "        N, C, H, W = x.shape\n",
    "        out_h = int(1 + (H - self.pool_h) / self.stride)\n",
    "        out_w = int(1 + (H - self.pool_w) / self.stride)\n",
    "        \n",
    "        # 전개 (1)\n",
    "        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)\n",
    "        col = col.reshape(-1, self.pool_h*self.pool_w)\n",
    "        \n",
    "        # 최댓값 (2)\n",
    "        out = np.max(col, axis=1)  # axis=0 : 열, axis=1 : 행\n",
    "        \n",
    "        # 성형 (3)\n",
    "        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)\n",
    "        \n",
    "        return out\n",
    "\n",
    "class SimpleConvNet:\n",
    "    def __init__(self, input_dim=(1, 28, 28), \n",
    "                 conv_param={'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1},\n",
    "                hidden_size=100, output_size=10, weight_init_std=0.01):\n",
    "        filter_num = conv_param['filter_num']\n",
    "        filter_size = conv_param['filter_size']\n",
    "        filter_pad = conv_param['pad']\n",
    "        filter_stride = conv_param['stride']\n",
    "        \n",
    "        input_size = input_dim[1]\n",
    "        \n",
    "        conv_output_size = (input_size - filter_size + 2*filter_pad) / filter_stride + 1\n",
    "        \n",
    "        pool_output_size = int(filter_num * (conv_output_size/2) * (conv_output_size/2))\n",
    "        self.params = {}\n",
    "        self.params['W1'] = weight_init_std * np.random.randn(filter_num, input_dim[0], filter_size, filter_size)\n",
    "        self.params['b1'] = np.zeros(filter_num)\n",
    "        self.params['W2'] = weight_init_std * np.random.randn(pool_output_size, hidden_size)\n",
    "        self.params['b2'] = np.zeros(hidden_size)\n",
    "        self.params['W3'] = weight_init_std * np.random.randn(hidden_size, output_size)\n",
    "        self.params['b3'] = np.zeros(output_size)\n",
    "        self.layers = OrderedDict()\n",
    "        self.layers['Conv1'] = Convolution(self.params['W1'], self.params['b1'],\n",
    "                                          conv_param['stride'], conv_param['pad'])\n",
    "        self.layers['Relu1'] = Relu()\n",
    "        self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)\n",
    "        \n",
    "        self.layers['Affine1'] = Affine(self.params['W2'], self.params['b2'])\n",
    "        self.layers['Relu2'] = Relu()\n",
    "        \n",
    "        self.layers['Affine2'] = Affine(self.params['W3'], self.params['b3'])\n",
    "        self.last_layer = SoftmaxWithLoss()\n",
    "    def predict(self, x):\n",
    "        for layer in self.layers.values():\n",
    "            x = layer.forward(x)\n",
    "        return x\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        y = self.predict(x)\n",
    "        return self.last_layer.forward(y, t)\n",
    "\n",
    "    def gradient(self, x, t):\n",
    "        # 순전파\n",
    "        self.loss(x, t)\n",
    "        \n",
    "        # 역전파\n",
    "        dout = 1\n",
    "        dout = self.last_layer.backward(dout)\n",
    "        \n",
    "        layers = list(self.layers.values())\n",
    "        layers.reverse()\n",
    "        \n",
    "        for layer in layers:\n",
    "            dout = layer.backward(dout)\n",
    "            \n",
    "        # 결과 저장\n",
    "        grads = {}\n",
    "        grads['W1'] = self.layers['Conv1'].dW\n",
    "        grads['b1'] = self.layers['Conv1'].db\n",
    "        grads['W2'] = self.layers['Affine1'].dW\n",
    "        grads['b2'] = self.layers['Affine1'].db\n",
    "        grads['W3'] = self.layers['Affine2'].dW\n",
    "        grads['b3'] = self.layers['Affine2'].db\n",
    "        \n",
    "        return grads   \n",
    "\n",
    "# load dataset\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "max_epochs = 20\n",
    "\n",
    "network = SimpleConvNet(input_dim=(1,28,28), \n",
    "                        conv_param = {'filter_num': 30, 'filter_size': 5, 'pad': 0, 'stride': 1},\n",
    "                        hidden_size=100, output_size=10, weight_init_std=0.01)\n",
    "                        \n",
    "trainer = Trainer(network, x_train, t_train, x_test, t_test,\n",
    "                  epochs=max_epochs, mini_batch_size=100,\n",
    "                  optimizer='Adam', optimizer_param={'lr': 0.001},\n",
    "                  evaluate_sample_num_per_epoch=1000)\n",
    "trainer.train()\n",
    "\n",
    "# 매개변수 보존\n",
    "network.save_params(\"params.pkl\")\n",
    "print(\"Saved Network Parameters!\")\n",
    "\n",
    "# 그래프 그리기\n",
    "markers = {'train': 'o', 'test': 's'}\n",
    "x = np.arange(max_epochs)\n",
    "plt.plot(x, trainer.train_acc_list, marker='o', label='train', markevery=2)\n",
    "plt.plot(x, trainer.test_acc_list, marker='s', label='test', markevery=2)\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"accuracy\")\n",
    "plt.ylim(0, 1.0)\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 1.3898 - acc: 0.5200 - precision: 0.6578 - recall: 0.3872\n",
      "1563/1563 [==============================] - 735s 470ms/step - loss: 1.6367 - acc: 0.4080 - precision: 0.6190 - recall: 0.1913 - val_loss: 1.3894 - val_acc: 0.5200 - val_precision: 0.6578 - val_recall: 0.3872\n",
      "Epoch 2/50\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 1.0724 - acc: 0.6361 - precision: 0.7413 - recall: 0.5468\n",
      "1563/1563 [==============================] - 789s 505ms/step - loss: 1.2247 - acc: 0.5627 - precision: 0.7246 - recall: 0.4002 - val_loss: 1.0723 - val_acc: 0.6361 - val_precision: 0.7413 - val_recall: 0.5468\n",
      "Epoch 3/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.9810 - acc: 0.6665 - precision: 0.7916 - recall: 0.5716\n",
      "1563/1563 [==============================] - 733s 469ms/step - loss: 1.0344 - acc: 0.6406 - precision: 0.7740 - recall: 0.5104 - val_loss: 0.9811 - val_acc: 0.6665 - val_precision: 0.7916 - val_recall: 0.5716\n",
      "Epoch 4/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.7802 - acc: 0.7353 - precision: 0.8262 - recall: 0.6502\n",
      "1563/1563 [==============================] - 659s 422ms/step - loss: 0.9246 - acc: 0.6864 - precision: 0.7996 - recall: 0.5753 - val_loss: 0.7797 - val_acc: 0.7353 - val_precision: 0.8262 - val_recall: 0.6502\n",
      "Epoch 5/50\n",
      "10000/10000 [==============================] - 20s 2ms/sample - loss: 0.8363 - acc: 0.7232 - precision: 0.7970 - recall: 0.6619s\n",
      "1563/1563 [==============================] - 692s 443ms/step - loss: 0.8448 - acc: 0.7117 - precision: 0.8137 - recall: 0.6149 - val_loss: 0.8368 - val_acc: 0.7232 - val_precision: 0.7970 - val_recall: 0.6619\n",
      "Epoch 6/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.6767 - acc: 0.7743 - precision: 0.8489 - recall: 0.7118s - loss: 0.6802 - acc: 0.7747 - precision: 0.8481 - re - ETA: 0s - loss: 0.6791 - acc: 0.7743 - precisio\n",
      "1563/1563 [==============================] - 681s 436ms/step - loss: 0.7841 - acc: 0.7324 - precision: 0.8242 - recall: 0.6438 - val_loss: 0.6767 - val_acc: 0.7743 - val_precision: 0.8489 - val_recall: 0.7118\n",
      "Epoch 7/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.6318 - acc: 0.7876 - precision: 0.8586 - recall: 0.7285\n",
      "1563/1563 [==============================] - 692s 443ms/step - loss: 0.7457 - acc: 0.7462 - precision: 0.8340 - recall: 0.6674 - val_loss: 0.6315 - val_acc: 0.7876 - val_precision: 0.8586 - val_recall: 0.7285\n",
      "Epoch 8/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.6339 - acc: 0.7863 - precision: 0.8520 - recall: 0.7278\n",
      "1563/1563 [==============================] - 663s 424ms/step - loss: 0.7069 - acc: 0.7606 - precision: 0.8436 - recall: 0.6860 - val_loss: 0.6341 - val_acc: 0.7863 - val_precision: 0.8520 - val_recall: 0.7278\n",
      "Epoch 9/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.5897 - acc: 0.8023 - precision: 0.8538 - recall: 0.7566\n",
      "1563/1563 [==============================] - 664s 425ms/step - loss: 0.6779 - acc: 0.7691 - precision: 0.8488 - recall: 0.6996 - val_loss: 0.5899 - val_acc: 0.8023 - val_precision: 0.8538 - val_recall: 0.7566\n",
      "Epoch 10/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.6057 - acc: 0.7962 - precision: 0.8555 - recall: 0.7459\n",
      "1563/1563 [==============================] - 698s 447ms/step - loss: 0.6476 - acc: 0.7826 - precision: 0.8540 - recall: 0.7162 - val_loss: 0.6060 - val_acc: 0.7962 - val_precision: 0.8555 - val_recall: 0.7459\n",
      "Epoch 11/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.5966 - acc: 0.8007 - precision: 0.8607 - recall: 0.7502\n",
      "1563/1563 [==============================] - 679s 434ms/step - loss: 0.6259 - acc: 0.7863 - precision: 0.8566 - recall: 0.7236 - val_loss: 0.5964 - val_acc: 0.8007 - val_precision: 0.8607 - val_recall: 0.7502\n",
      "Epoch 12/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.5359 - acc: 0.8217 - precision: 0.8778 - recall: 0.7697\n",
      "1563/1563 [==============================] - 670s 429ms/step - loss: 0.6078 - acc: 0.7944 - precision: 0.8615 - recall: 0.7333 - val_loss: 0.5359 - val_acc: 0.8217 - val_precision: 0.8778 - val_recall: 0.7697\n",
      "Epoch 13/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.5914 - acc: 0.8046 - precision: 0.8548 - recall: 0.7589\n",
      "1563/1563 [==============================] - 664s 425ms/step - loss: 0.5916 - acc: 0.7997 - precision: 0.8654 - recall: 0.7415 - val_loss: 0.5910 - val_acc: 0.8046 - val_precision: 0.8548 - val_recall: 0.7589\n",
      "Epoch 14/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.5874 - acc: 0.8071 - precision: 0.8565 - recall: 0.7637\n",
      "1563/1563 [==============================] - 664s 425ms/step - loss: 0.5753 - acc: 0.8049 - precision: 0.8669 - recall: 0.7475 - val_loss: 0.5874 - val_acc: 0.8071 - val_precision: 0.8565 - val_recall: 0.7637\n",
      "Epoch 15/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.5270 - acc: 0.8289 - precision: 0.8740 - recall: 0.7878s - loss: 0.5252 - acc: 0.8289 - precisio\n",
      "1563/1563 [==============================] - 663s 424ms/step - loss: 0.5605 - acc: 0.8104 - precision: 0.8686 - recall: 0.7562 - val_loss: 0.5267 - val_acc: 0.8289 - val_precision: 0.8740 - val_recall: 0.7878\n",
      "Epoch 16/50\n",
      "10000/10000 [==============================] - 19s 2ms/sample - loss: 0.4854 - acc: 0.8407 - precision: 0.8804 - recall: 0.8059\n",
      "1563/1563 [==============================] - 671s 429ms/step - loss: 0.5422 - acc: 0.8162 - precision: 0.8756 - recall: 0.7648 - val_loss: 0.4853 - val_acc: 0.8407 - val_precision: 0.8804 - val_recall: 0.8059\n",
      "Epoch 17/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.5302 - acc: 0.8251 - precision: 0.8746 - recall: 0.7803\n",
      "1563/1563 [==============================] - 666s 426ms/step - loss: 0.5402 - acc: 0.8179 - precision: 0.8754 - recall: 0.7651 - val_loss: 0.5304 - val_acc: 0.8251 - val_precision: 0.8746 - val_recall: 0.7803\n",
      "Epoch 18/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.4637 - acc: 0.8471 - precision: 0.8834 - recall: 0.8111\n",
      "1563/1563 [==============================] - 665s 426ms/step - loss: 0.5146 - acc: 0.8232 - precision: 0.8799 - recall: 0.7761 - val_loss: 0.4635 - val_acc: 0.8471 - val_precision: 0.8834 - val_recall: 0.8111\n",
      "Epoch 19/50\n",
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.4589 - acc: 0.8483 - precision: 0.8864 - recall: 0.8149\n",
      "1563/1563 [==============================] - 670s 429ms/step - loss: 0.5112 - acc: 0.8255 - precision: 0.8790 - recall: 0.7768 - val_loss: 0.4589 - val_acc: 0.8483 - val_precision: 0.8864 - val_recall: 0.8149\n",
      "Epoch 20/50\n",
      " 121/1563 [=>............................] - ETA: 10:54 - loss: 0.4823 - acc: 0.8383 - precision: 0.8848 - recall: 0.7952"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-aa310c052622>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     67\u001b[0m               \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m               \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_cat_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;31m#               callbacks=[early_stop],\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m#               batch_size=batch_size,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    735\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    736\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 737\u001b[1;33m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    738\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m     \u001b[1;31m# Legacy support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1424\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1426\u001b[1;33m         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m   def evaluate_generator(self,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, **kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m       \u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m       \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1189\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_fit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1191\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1193\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Flatten, Dropout, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "# Scale the data\n",
    "X_train = X_train / 255\n",
    "X_test = X_test / 255\n",
    "\n",
    "y_cat_train = to_categorical(y_train, 10)\n",
    "y_cat_test = to_categorical(y_test, 10)\n",
    "\n",
    "#Create Model\n",
    "model = Sequential()\n",
    "\n",
    "# Convolutional Layer\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=32, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "# Pooling layer\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "# Dropout layers\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=64, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(filters=128, kernel_size=(3, 3), input_shape=(32, 32, 3), activation='relu', padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "METRICS = [\n",
    "    'accuracy',\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall')\n",
    "]\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=METRICS)\n",
    "\n",
    "#Augment Data\n",
    "batch_size = 32\n",
    "data_generator = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "train_generator = data_generator.flow(X_train, y_cat_train, batch_size)\n",
    "steps_per_epoch = X_train.shape[0] // batch_size\n",
    "\n",
    "r = model.fit(train_generator, \n",
    "              epochs=50,\n",
    "              steps_per_epoch=steps_per_epoch,\n",
    "              validation_data=(X_test, y_cat_test)\n",
    "             )\n",
    "\n",
    "evaluation = model.evaluate(X_test, y_cat_test)\n",
    "print(f'Test Accuracy : {evaluation[1] * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 18s 2ms/sample - loss: 0.4804 - acc: 0.8391 - precision: 0.8866 - recall: 0.7975\n",
      "Test Accuracy : 83.91%\n"
     ]
    }
   ],
   "source": [
    "evaluation = model.evaluate(X_test, y_cat_test)\n",
    "print(f'Test Accuracy : {evaluation[1] * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
