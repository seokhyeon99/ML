{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Activation,Flatten,Dropout\n",
    "from keras.layers import Conv2D,MaxPooling2D\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "import os # operating system interfaces\n",
    "# Set data folder\n",
    "img_dir_train_n = 'D:\\\\data\\\\chest_xray\\\\train\\\\NORMAL'\n",
    "img_dir_val_n = 'D:\\\\data\\\\chest_xray\\\\val\\\\NORMAL'\n",
    "img_dir_test_n = 'D:\\\\data\\\\chest_xray\\\\test\\\\NORMAL'\n",
    "img_dir_train_p = 'D:\\\\data\\\\chest_xray\\\\train\\\\PNEUMONIA'\n",
    "img_dir_val_p = 'D:\\\\data\\\\chest_xray\\\\val\\\\PNEUMONIA'\n",
    "img_dir_test_p = 'D:\\\\data\\\\chest_xray\\\\test\\\\PNEUMONIA'\n",
    "# get file names\n",
    "flist_train_n = os.listdir(img_dir_train_n)\n",
    "flist_val_n = os.listdir(img_dir_val_n)\n",
    "flist_test_n = os.listdir(img_dir_test_n)\n",
    "flist_train_p = os.listdir(img_dir_train_p)\n",
    "flist_val_p = os.listdir(img_dir_val_p)\n",
    "flist_test_p = os.listdir(img_dir_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the image into a 4D array using\n",
    "#keras.preprocessing\n",
    "from keras.preprocessing import image\n",
    "# load train images\n",
    "X_train = np.zeros(shape=((len(flist_train_p)+len(flist_train_n),200,200,3)))\n",
    "y_train = np.zeros(shape=(len(flist_train_p)+len(flist_train_n)))\n",
    "for idx , fname in enumerate(flist_train_n):\n",
    "    img_path = os.path.join(img_dir_train_n , fname)\n",
    "    img = image.load_img(img_path , target_size=(200,200))\n",
    "    img_array_train = image.img_to_array(img)\n",
    "    img_array_train = np.expand_dims(img_array_train , axis=0)\n",
    "    X_train[idx] = img_array_train\n",
    "    y_train[idx] = 0\n",
    "for idx , fname in enumerate(flist_train_p):\n",
    "    img_path = os.path.join(img_dir_train_p , fname)\n",
    "    img = image.load_img(img_path , target_size=(200,200))\n",
    "    img_array_train = image.img_to_array(img)\n",
    "    img_array_train = np.expand_dims(img_array_train , axis=0)\n",
    "    X_train[idx+len(flist_test_n)] = img_array_train\n",
    "    y_train[idx+len(flist_test_n)] = 1\n",
    "# scaling into [0, 1]\n",
    "X_train = X_train / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the image into a 4D array using\n",
    "#keras.preprocessing\n",
    "from keras.preprocessing import image\n",
    "# load train images\n",
    "X_val = np.zeros(shape=((len(flist_val_p)+len(flist_val_n),200,200,3)))\n",
    "y_val = np.zeros(shape=(len(flist_val_p)+len(flist_val_n)))\n",
    "for idx , fname in enumerate(flist_val_n):\n",
    "    img_path = os.path.join(img_dir_val_n , fname)\n",
    "    img = image.load_img(img_path , target_size=(200,200))\n",
    "    img_array_val = image.img_to_array(img)\n",
    "    img_array_val = np.expand_dims(img_array_val , axis=0)\n",
    "    X_val[idx] = img_array_val\n",
    "    y_val[idx] = 0\n",
    "for idx , fname in enumerate(flist_val_p):\n",
    "    img_path = os.path.join(img_dir_val_p , fname)\n",
    "    img = image.load_img(img_path , target_size=(200,200))\n",
    "    img_array_val = image.img_to_array(img)\n",
    "    img_array_val = np.expand_dims(img_array_val , axis=0)\n",
    "    X_val[idx+len(flist_val_n)] = img_array_val\n",
    "    y_val[idx+len(flist_val_n)] = 1\n",
    "# scaling into [0, 1]\n",
    "X_val = X_val / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess the image into a 4D array using\n",
    "#keras.preprocessing\n",
    "from keras.preprocessing import image\n",
    "# load train images\n",
    "X_test = np.zeros(shape=((len(flist_test_p)+len(flist_test_n),200,200,3)))\n",
    "y_test = np.zeros(shape=(len(flist_test_p)+len(flist_test_n)))\n",
    "for idx , fname in enumerate(flist_test_n):\n",
    "    img_path = os.path.join(img_dir_test_n , fname)\n",
    "    img = image.load_img(img_path , target_size=(200,200))\n",
    "    img_array_test = image.img_to_array(img)\n",
    "    img_array_test = np.expand_dims(img_array_test , axis=0)\n",
    "    X_test[idx] = img_array_test\n",
    "    y_test[idx] = 0\n",
    "for idx , fname in enumerate(flist_test_p):\n",
    "    img_path = os.path.join(img_dir_test_p , fname)\n",
    "    img = image.load_img(img_path , target_size=(200,200))\n",
    "    img_array_test = image.img_to_array(img)\n",
    "    img_array_test = np.expand_dims(img_array_test , axis=0)\n",
    "    X_test[idx+len(flist_test_n)] = img_array_test\n",
    "    y_test[idx+len(flist_test_n)] = 1\n",
    "# scaling into [0, 1]\n",
    "X_test = X_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = ImageDataGenerator(\n",
    "                                    rotation_range=2, \n",
    "                                    horizontal_flip=True,\n",
    "                                    zoom_range=.1 )\n",
    "\n",
    "val_generator = ImageDataGenerator(\n",
    "                                    rotation_range=2, \n",
    "                                    horizontal_flip=True,\n",
    "                                    zoom_range=.1)\n",
    "\n",
    "test_generator = ImageDataGenerator(\n",
    "                                    rotation_range=2, \n",
    "                                    horizontal_flip= True,\n",
    "                                    zoom_range=.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.fit(X_train)\n",
    "val_generator.fit(X_val)\n",
    "test_generator.fit(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_4=Sequential()\n",
    "\n",
    "#The first CNN layer followed by Relu and MaxPooling layers\n",
    "model_4.add(Conv2D(32,(3,3),input_shape=(200,200,3)))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#The second convolution layer followed by Relu and MaxPooling layers\n",
    "model_4.add(Conv2D(64,(3,3)))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_4.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "#The second convolution layer followed by Relu and MaxPooling layers\n",
    "model_4.add(Conv2D(128,(3,3)))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "#The Third convolution layer followed by Relu and MaxPooling layers\n",
    "model_4.add(Conv2D(256,(3,3)))\n",
    "model_4.add(Activation('relu'))\n",
    "model_4.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model_4.add(Dropout(0.3))\n",
    "\n",
    "\n",
    "#Flatten layer to stack the output convolutions from convolution layer\n",
    "model_4.add(Flatten())\n",
    "\n",
    "#Dense layer of 256 neurons\n",
    "model_4.add(Dense(256,activation='relu'))\n",
    "model_4.add(Dropout(0.5))\n",
    "\n",
    "model_4.add(Dense(1,activation='sigmoid'))\n",
    "#The Final layer with two outputs for two categories\n",
    "\n",
    "model_4.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\이석현\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 5216 samples, validate on 16 samples\n",
      "Epoch 1/10\n",
      "5216/5216 [==============================] - 332s 64ms/step - loss: 0.2033 - accuracy: 0.9484 - val_loss: 4.6230 - val_accuracy: 0.1250\n",
      "Epoch 2/10\n",
      "5216/5216 [==============================] - 323s 62ms/step - loss: 0.1140 - accuracy: 0.9628 - val_loss: 3.9467 - val_accuracy: 0.0625\n",
      "Epoch 3/10\n",
      "5216/5216 [==============================] - 319s 61ms/step - loss: 0.0791 - accuracy: 0.9737 - val_loss: 3.3956 - val_accuracy: 0.3125\n",
      "Epoch 4/10\n",
      "5216/5216 [==============================] - 319s 61ms/step - loss: 0.0539 - accuracy: 0.9791 - val_loss: 4.5420 - val_accuracy: 0.1250\n",
      "Epoch 5/10\n",
      "5216/5216 [==============================] - 317s 61ms/step - loss: 0.0532 - accuracy: 0.9824 - val_loss: 5.7563 - val_accuracy: 0.1250\n",
      "Epoch 6/10\n",
      "5216/5216 [==============================] - 317s 61ms/step - loss: 0.0367 - accuracy: 0.9872 - val_loss: 5.9196 - val_accuracy: 0.1250\n",
      "Epoch 7/10\n",
      "5216/5216 [==============================] - 324s 62ms/step - loss: 0.0324 - accuracy: 0.9881 - val_loss: 6.9489 - val_accuracy: 0.0625\n",
      "Epoch 8/10\n",
      "5216/5216 [==============================] - 315s 60ms/step - loss: 0.0312 - accuracy: 0.9885 - val_loss: 4.4750 - val_accuracy: 0.4375\n",
      "Epoch 9/10\n",
      "5216/5216 [==============================] - 314s 60ms/step - loss: 0.0297 - accuracy: 0.9895 - val_loss: 6.5214 - val_accuracy: 0.1250\n",
      "Epoch 10/10\n",
      "5216/5216 [==============================] - 313s 60ms/step - loss: 0.0239 - accuracy: 0.9919 - val_loss: 7.4934 - val_accuracy: 0.0625\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 32\n",
    "disp = model_4.fit(X_train , y_train ,batch_size = batch_size, epochs=epochs, verbose=1, validation_data = (X_val , y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 11.076930730770796\n",
      "Test accuracy: 0.07532051205635071\n"
     ]
    }
   ],
   "source": [
    "score = model_4.evaluate(X_test , y_test , verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "============================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\이석현\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model_2=Sequential()\n",
    "\n",
    "#The first CNN layer followed by Relu and MaxPooling layers\n",
    "model_2.add(Conv2D(32,(3,3),input_shape=(200,200,3)))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "#The second convolution layer followed by Relu and MaxPooling layers\n",
    "model_2.add(Conv2D(64,(3,3)))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "\n",
    "#The second convolution layer followed by Relu and MaxPooling layers\n",
    "model_2.add(Conv2D(128,(3,3)))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "\n",
    "'''\n",
    "#The Third convolution layer followed by Relu and MaxPooling layers\n",
    "model_2.add(Conv2D(256,(3,3)))\n",
    "model_2.add(Activation('relu'))\n",
    "model_2.add(MaxPooling2D(pool_size=(2,2)))\n",
    "'''\n",
    "\n",
    "#Flatten layer to stack the output convolutions from convolution layer\n",
    "model_2.add(Flatten())\n",
    "\n",
    "#Dense layer of 256 neurons\n",
    "model_2.add(Dense(128,activation='relu'))\n",
    "model_2.add(Dropout(0.5))\n",
    "\n",
    "model_2.add(Dense(1,activation='sigmoid'))\n",
    "#The Final layer with two outputs for two categories\n",
    "\n",
    "model_2.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\이석현\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 5116 samples, validate on 16 samples\n",
      "Epoch 1/20\n",
      "5116/5116 [==============================] - 283s 55ms/step - loss: 0.1735 - accuracy: 0.9517 - val_loss: 3.5227 - val_accuracy: 0.0625\n",
      "Epoch 2/20\n",
      "5116/5116 [==============================] - 265s 52ms/step - loss: 0.0979 - accuracy: 0.9629 - val_loss: 4.2489 - val_accuracy: 0.0625\n",
      "Epoch 3/20\n",
      "5116/5116 [==============================] - 269s 53ms/step - loss: 0.0672 - accuracy: 0.9728 - val_loss: 4.7896 - val_accuracy: 0.1875\n",
      "Epoch 4/20\n",
      "5116/5116 [==============================] - 268s 52ms/step - loss: 0.0542 - accuracy: 0.9806 - val_loss: 4.7724 - val_accuracy: 0.1250\n",
      "Epoch 5/20\n",
      "5116/5116 [==============================] - 270s 53ms/step - loss: 0.0455 - accuracy: 0.9840 - val_loss: 5.2903 - val_accuracy: 0.1875\n",
      "Epoch 6/20\n",
      "5116/5116 [==============================] - 262s 51ms/step - loss: 0.0327 - accuracy: 0.9867 - val_loss: 7.8210 - val_accuracy: 0.1250\n",
      "Epoch 7/20\n",
      "5116/5116 [==============================] - 263s 51ms/step - loss: 0.0386 - accuracy: 0.9848 - val_loss: 6.0413 - val_accuracy: 0.1875\n",
      "Epoch 8/20\n",
      "5116/5116 [==============================] - 272s 53ms/step - loss: 0.0271 - accuracy: 0.9896 - val_loss: 7.0043 - val_accuracy: 0.2500\n",
      "Epoch 9/20\n",
      "5116/5116 [==============================] - 265s 52ms/step - loss: 0.0206 - accuracy: 0.9924 - val_loss: 9.4866 - val_accuracy: 0.1250\n",
      "Epoch 10/20\n",
      "5116/5116 [==============================] - 265s 52ms/step - loss: 0.0197 - accuracy: 0.9920 - val_loss: 10.2088 - val_accuracy: 0.1250\n",
      "Epoch 11/20\n",
      "5116/5116 [==============================] - 264s 52ms/step - loss: 0.0113 - accuracy: 0.9963 - val_loss: 9.6124 - val_accuracy: 0.1875\n",
      "Epoch 12/20\n",
      "5116/5116 [==============================] - 273s 53ms/step - loss: 0.0155 - accuracy: 0.9939 - val_loss: 10.7067 - val_accuracy: 0.1250\n",
      "Epoch 13/20\n",
      "5116/5116 [==============================] - 266s 52ms/step - loss: 0.0156 - accuracy: 0.9941 - val_loss: 9.2827 - val_accuracy: 0.1875\n",
      "Epoch 14/20\n",
      "5116/5116 [==============================] - 255s 50ms/step - loss: 0.0109 - accuracy: 0.9959 - val_loss: 9.9587 - val_accuracy: 0.1875\n",
      "Epoch 15/20\n",
      " 416/5116 [=>............................] - ETA: 3:48 - loss: 0.0041 - accuracy: 1.0000"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-3d479d3c0f3a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdisp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "batch_size = 32\n",
    "disp = model_2.fit(X_train , y_train ,batch_size = batch_size, epochs=epochs, verbose=1, validation_data = (X_val , y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(disp.histroy['accuracy'])\n",
    "plt.plot(disp.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 2.2989604870823293\n",
      "Test accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "score = model_2.evaluate(X_test , y_test , verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model_2.to_json()\n",
    "with open(\"nineth_model.json\", \"w\") as json_file : \n",
    "    json_file.write(model_json)\n",
    "model.save_weights(\"nineth_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\이석현\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open(\"eighth_model.json\", \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"eighth_model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 4.423185204848265\n",
      "Test accuracy: 0.29487180709838867\n"
     ]
    }
   ],
   "source": [
    "score = loaded_model.evaluate(X_test , y_test , verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 4.423185204848265\n",
      "Test accuracy: 0.29487180709838867\n"
     ]
    }
   ],
   "source": [
    "score = loaded_model.evaluate(X_test , y_test , verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 624 samples, validate on 16 samples\n",
      "Epoch 1/1\n",
      "128/624 [=====>........................] - ETA: 23s - loss: 0.3050 - accuracy: 0.8906"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-7a9d5bf1cc7c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdisp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 1\n",
    "batch_size = 32\n",
    "disp = model_2.fit(X_test , y_test ,batch_size = batch_size, epochs=epochs, verbose=1, validation_data = (X_val , y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2072487571873726\n",
      "Test accuracy: 0.9278846383094788\n"
     ]
    }
   ],
   "source": [
    "score = model_2.evaluate(X_test , y_test , verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_json = model_2.to_json()\n",
    "with open(\"nineth_model.json\", \"w\") as json_file : \n",
    "    json_file.write(model_json)\n",
    "model_2.save_weights(\"nineth_model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open(\"nineth_model.json\", \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "loaded_model.load_weights(\"nineth_model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "loaded_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2072487571873726\n",
      "Test accuracy: 0.9278846383094788\n"
     ]
    }
   ],
   "source": [
    "score = loaded_model.evaluate(X_test , y_test , verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
