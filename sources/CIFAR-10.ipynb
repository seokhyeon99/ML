{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for developing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import Sequential\n",
    "from keras.applications import VGG19\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD,Adam\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.layers import Flatten,Dense,BatchNormalization,Activation,Dropout\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "(x_train,y_train),(x_test,y_test)=cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_val,y_train,y_val=train_test_split(x_train,y_train,test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((35000, 32, 32, 3), (35000, 1))\n",
      "((15000, 32, 32, 3), (15000, 1))\n",
      "((10000, 32, 32, 3), (10000, 1))\n"
     ]
    }
   ],
   "source": [
    "print((x_train.shape,y_train.shape))\n",
    "print((x_val.shape,y_val.shape))\n",
    "print((x_test.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 74, 143, 201],\n",
       "        [ 71, 140, 200],\n",
       "        [ 69, 139, 198],\n",
       "        ...,\n",
       "        [192, 218, 234],\n",
       "        [193, 208, 229],\n",
       "        [184, 200, 219]],\n",
       "\n",
       "       [[ 78, 140, 194],\n",
       "        [ 77, 140, 194],\n",
       "        [ 76, 140, 193],\n",
       "        ...,\n",
       "        [226, 240, 233],\n",
       "        [214, 224, 238],\n",
       "        [189, 208, 227]],\n",
       "\n",
       "       [[ 91, 149, 198],\n",
       "        [ 95, 153, 203],\n",
       "        [ 98, 156, 206],\n",
       "        ...,\n",
       "        [207, 231, 236],\n",
       "        [181, 216, 231],\n",
       "        [149, 188, 211]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 88, 131, 167],\n",
       "        [ 83, 128, 165],\n",
       "        [ 75, 122, 161],\n",
       "        ...,\n",
       "        [ 86, 129, 175],\n",
       "        [ 81, 124, 173],\n",
       "        [ 81, 121, 170]],\n",
       "\n",
       "       [[ 74, 116, 155],\n",
       "        [ 69, 113, 154],\n",
       "        [ 62, 108, 151],\n",
       "        ...,\n",
       "        [ 76, 120, 166],\n",
       "        [ 67, 112, 161],\n",
       "        [ 66, 108, 157]],\n",
       "\n",
       "       [[ 63, 105, 147],\n",
       "        [ 62, 106, 150],\n",
       "        [ 56, 102, 148],\n",
       "        ...,\n",
       "        [ 62, 104, 151],\n",
       "        [ 54, 100, 149],\n",
       "        [ 56,  99, 147]]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=to_categorical(y_train)\n",
    "y_val=to_categorical(y_val)\n",
    "y_test=to_categorical(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((35000, 32, 32, 3), (35000, 10))\n",
      "((15000, 32, 32, 3), (15000, 10))\n",
      "((10000, 32, 32, 3), (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "print((x_train.shape,y_train.shape))\n",
    "print((x_val.shape,y_val.shape))\n",
    "print((x_test.shape,y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = ImageDataGenerator(\n",
    "                                    rotation_range=2, \n",
    "                                    horizontal_flip=True,\n",
    "                                    zoom_range=.1 )\n",
    "\n",
    "val_generator = ImageDataGenerator(\n",
    "                                    rotation_range=2, \n",
    "                                    horizontal_flip=True,\n",
    "                                    zoom_range=.1)\n",
    "\n",
    "test_generator = ImageDataGenerator(\n",
    "                                    rotation_range=2, \n",
    "                                    horizontal_flip= True,\n",
    "                                    zoom_range=.1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator.fit(x_train)\n",
    "val_generator.fit(x_val)\n",
    "test_generator.fit(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrr= ReduceLROnPlateau(\n",
    "                       monitor='val_acc', #Metric to be measured\n",
    "                       factor=.01, #Factor by which learning rate will be reduced\n",
    "                       patience=3,  #No. of epochs after which if there is no improvement in the val_acc, the learning rate is reduced\n",
    "                       min_lr=1e-5) #The minimum learning rate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\이석현\\AppData\\Roaming\\Python\\Python37\\site-packages\\keras_applications\\resnet50.py:265: UserWarning: The output shape of `ResNet50(include_top=False)` has been changed since Keras 2.2.0.\n",
      "  warnings.warn('The output shape of `ResNet50(include_top=False)` '\n"
     ]
    }
   ],
   "source": [
    "'The first base model used is VGG19. The pretrained weights from the imagenet challenge are used'\n",
    "base_model_1 = VGG19(include_top=False,weights='imagenet',input_shape=(32,32,3),classes=y_train.shape[1])\n",
    "\n",
    "'For the 2nd base model we will use Resnet 50 and compare the performance against the previous one.The hypothesis is that Resnet 50 should perform better because of its deeper architecture'\n",
    "base_model_2 = ResNet50(include_top=False,weights='imagenet',input_shape=(32,32,3),classes=y_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1= Sequential()\n",
    "model_1.add(base_model_1) #Adds the base model (in this case vgg19 to model_1)\n",
    "model_1.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 1, 1, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.add(Dense(1024,activation=('relu'),input_dim=512))\n",
    "model_1.add(Dense(512,activation=('relu'))) \n",
    "model_1.add(Dense(256,activation=('relu'))) \n",
    "#model_1.add(Dropout(.3))#Adding a dropout layer that will randomly drop 30% of the weights\n",
    "model_1.add(Dense(128,activation=('relu')))\n",
    "#model_1.add(Dropout(.2))\n",
    "model_1.add(Dense(10,activation=('softmax')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 1, 1, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py:297: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    }
   ],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 100\n",
    "epochs=50\n",
    "\n",
    "learn_rate=.001\n",
    "\n",
    "sgd=SGD(lr=learn_rate,momentum=.9,nesterov=False)\n",
    "adam=Adam(lr=learn_rate, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(optimizer=sgd,loss='categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "350/350 [==============================] - 2046s 6s/step - loss: 1.7186 - accuracy: 0.3545 - val_loss: 1.2192 - val_accuracy: 0.5733\n",
      "Epoch 2/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\Anaconda3\\lib\\site-packages\\keras\\callbacks\\callbacks.py:1042: RuntimeWarning: Reduce LR on plateau conditioned on metric `val_acc` which is not available. Available metrics are: val_loss,val_accuracy,loss,accuracy,lr\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350/350 [==============================] - 2065s 6s/step - loss: 0.9338 - accuracy: 0.6791 - val_loss: 0.8216 - val_accuracy: 0.7285\n",
      "Epoch 3/50\n",
      "350/350 [==============================] - 2053s 6s/step - loss: 0.7221 - accuracy: 0.7540 - val_loss: 0.8813 - val_accuracy: 0.7663\n",
      "Epoch 4/50\n",
      "350/350 [==============================] - 2037s 6s/step - loss: 0.6059 - accuracy: 0.7946 - val_loss: 0.6699 - val_accuracy: 0.7863\n",
      "Epoch 5/50\n",
      "350/350 [==============================] - 2004s 6s/step - loss: 0.5278 - accuracy: 0.8183 - val_loss: 0.5166 - val_accuracy: 0.7950\n",
      "Epoch 6/50\n",
      "350/350 [==============================] - 5482s 16s/step - loss: 0.4728 - accuracy: 0.8376 - val_loss: 0.6859 - val_accuracy: 0.8066\n",
      "Epoch 7/50\n",
      "350/350 [==============================] - 1978s 6s/step - loss: 0.4171 - accuracy: 0.8553 - val_loss: 0.4792 - val_accuracy: 0.8196\n",
      "Epoch 8/50\n",
      "350/350 [==============================] - 1981s 6s/step - loss: 0.3774 - accuracy: 0.8697 - val_loss: 0.5425 - val_accuracy: 0.8274\n",
      "Epoch 9/50\n",
      "350/350 [==============================] - 1981s 6s/step - loss: 0.3392 - accuracy: 0.8831 - val_loss: 0.3551 - val_accuracy: 0.8229\n",
      "Epoch 10/50\n",
      "350/350 [==============================] - 1957s 6s/step - loss: 0.3136 - accuracy: 0.8901 - val_loss: 0.5131 - val_accuracy: 0.8333\n",
      "Epoch 11/50\n",
      "350/350 [==============================] - 1914s 5s/step - loss: 0.2852 - accuracy: 0.9010 - val_loss: 0.5359 - val_accuracy: 0.8280\n",
      "Epoch 12/50\n",
      "350/350 [==============================] - 1960s 6s/step - loss: 0.2554 - accuracy: 0.9115 - val_loss: 0.7225 - val_accuracy: 0.8293\n",
      "Epoch 13/50\n",
      "350/350 [==============================] - 1978s 6s/step - loss: 0.2349 - accuracy: 0.9184 - val_loss: 0.7876 - val_accuracy: 0.8242\n",
      "Epoch 14/50\n",
      "350/350 [==============================] - 1979s 6s/step - loss: 0.2136 - accuracy: 0.9253 - val_loss: 0.5462 - val_accuracy: 0.8382\n",
      "Epoch 15/50\n",
      "350/350 [==============================] - 1984s 6s/step - loss: 0.1873 - accuracy: 0.9342 - val_loss: 0.7476 - val_accuracy: 0.8338\n",
      "Epoch 16/50\n",
      "350/350 [==============================] - 1976s 6s/step - loss: 0.1829 - accuracy: 0.9361 - val_loss: 0.4588 - val_accuracy: 0.8390\n",
      "Epoch 17/50\n",
      "350/350 [==============================] - 2000s 6s/step - loss: 0.1622 - accuracy: 0.9436 - val_loss: 0.3397 - val_accuracy: 0.8436\n",
      "Epoch 18/50\n",
      "350/350 [==============================] - 1981s 6s/step - loss: 0.1368 - accuracy: 0.9532 - val_loss: 0.3302 - val_accuracy: 0.8424\n",
      "Epoch 19/50\n",
      "350/350 [==============================] - 1992s 6s/step - loss: 0.1274 - accuracy: 0.9567 - val_loss: 0.4718 - val_accuracy: 0.8380\n",
      "Epoch 20/50\n",
      "350/350 [==============================] - 2024s 6s/step - loss: 0.1149 - accuracy: 0.9604 - val_loss: 0.6879 - val_accuracy: 0.8354\n",
      "Epoch 21/50\n",
      "350/350 [==============================] - 2233s 6s/step - loss: 0.1108 - accuracy: 0.9623 - val_loss: 0.6828 - val_accuracy: 0.8401\n",
      "Epoch 22/50\n",
      "350/350 [==============================] - 2118s 6s/step - loss: 0.0959 - accuracy: 0.9663 - val_loss: 0.9058 - val_accuracy: 0.8356\n",
      "Epoch 23/50\n",
      "350/350 [==============================] - 2092s 6s/step - loss: 0.0910 - accuracy: 0.9686 - val_loss: 0.3938 - val_accuracy: 0.8496\n",
      "Epoch 24/50\n",
      "350/350 [==============================] - 1961s 6s/step - loss: 0.0759 - accuracy: 0.9742 - val_loss: 1.0248 - val_accuracy: 0.8423\n",
      "Epoch 25/50\n",
      "350/350 [==============================] - 1957s 6s/step - loss: 0.0737 - accuracy: 0.9750 - val_loss: 0.5091 - val_accuracy: 0.8446\n",
      "Epoch 26/50\n",
      "350/350 [==============================] - 1957s 6s/step - loss: 0.0690 - accuracy: 0.9768 - val_loss: 0.5266 - val_accuracy: 0.8350\n",
      "Epoch 27/50\n",
      "350/350 [==============================] - 1992s 6s/step - loss: 0.0637 - accuracy: 0.9784 - val_loss: 0.5258 - val_accuracy: 0.8472\n",
      "Epoch 28/50\n",
      "350/350 [==============================] - 2168s 6s/step - loss: 0.0657 - accuracy: 0.9783 - val_loss: 0.5565 - val_accuracy: 0.8404\n",
      "Epoch 29/50\n",
      "350/350 [==============================] - 2129s 6s/step - loss: 0.0589 - accuracy: 0.9803 - val_loss: 0.9658 - val_accuracy: 0.8379\n",
      "Epoch 30/50\n",
      "350/350 [==============================] - 2164s 6s/step - loss: 0.0557 - accuracy: 0.9813 - val_loss: 1.0899 - val_accuracy: 0.8385\n",
      "Epoch 31/50\n",
      "350/350 [==============================] - 2117s 6s/step - loss: 0.0554 - accuracy: 0.9805 - val_loss: 1.1894 - val_accuracy: 0.8412\n",
      "Epoch 32/50\n",
      "350/350 [==============================] - 1958s 6s/step - loss: 0.0461 - accuracy: 0.9845 - val_loss: 0.8109 - val_accuracy: 0.8471\n",
      "Epoch 33/50\n",
      "350/350 [==============================] - 2081s 6s/step - loss: 0.0442 - accuracy: 0.9854 - val_loss: 0.8126 - val_accuracy: 0.8432\n",
      "Epoch 34/50\n",
      "350/350 [==============================] - 2025s 6s/step - loss: 0.0417 - accuracy: 0.9865 - val_loss: 0.4992 - val_accuracy: 0.8497\n",
      "Epoch 35/50\n",
      "  1/350 [..............................] - ETA: 33:29 - loss: 0.0305 - accuracy: 0.9800"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-51241dcc88e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m                       \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m//\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                       \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mval_generator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m250\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m                       callbacks=[lrr],verbose=1)\n\u001b[0m",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1732\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1733\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 220\u001b[1;33m                                             reset_metrics=False)\n\u001b[0m\u001b[0;32m    221\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Python\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[0;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_1.fit_generator(train_generator.flow(x_train,y_train,batch_size=batch_size),\n",
    "                      epochs=epochs,\n",
    "                      steps_per_epoch=x_train.shape[0]//batch_size,\n",
    "                      validation_data=val_generator.flow(x_val,y_val,batch_size=batch_size),validation_steps=250,\n",
    "                      callbacks=[lrr],verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.save('cifar10_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_json = model_1.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file : \n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "model_1.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "json_file = open(\"model.json\", \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "vgg19 (Model)                (None, 1, 1, 512)         20024384  \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               524800    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 128)               32896     \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                1290      \n",
      "=================================================================\n",
      "Total params: 21,240,010\n",
      "Trainable params: 21,240,010\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-ffd1ae7ce4dd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s : %.2f%%\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mloaded_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "score = loaded_model.evaluate(x_train, y_train, verbose=0)\n",
    "\n",
    "print(\"%s : %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://static.addtoany.com/images/dracaena-cinnabari.jpg\"\n",
    "print(os.system(\"curl\"))\n",
    "os.system(\"curl \" + url + \" > test.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "im = cv2.imread('test2.jpg') #사진 읽어들이기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB) #색공간 변환\n",
    "im = cv2.resize(im, (32,32)) #사이즈 조정\n",
    "\n",
    "#이미지 출력\n",
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(im)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.28627451 0.48235294 0.72941176]\n",
      "   [0.29019608 0.49019608 0.74509804]\n",
      "   [0.29411765 0.49019608 0.72941176]\n",
      "   ...\n",
      "   [0.34509804 0.50196078 0.70196078]\n",
      "   [0.4        0.52941176 0.70588235]\n",
      "   [0.34509804 0.50196078 0.70196078]]\n",
      "\n",
      "  [[0.29019608 0.49019608 0.7372549 ]\n",
      "   [0.28627451 0.49803922 0.74117647]\n",
      "   [0.29019608 0.50196078 0.74509804]\n",
      "   ...\n",
      "   [0.39215686 0.5372549  0.71372549]\n",
      "   [0.4        0.54509804 0.72156863]\n",
      "   [0.3372549  0.49803922 0.70196078]]\n",
      "\n",
      "  [[0.31764706 0.50980392 0.74117647]\n",
      "   [0.29803922 0.50980392 0.74509804]\n",
      "   [0.32156863 0.49803922 0.72156863]\n",
      "   ...\n",
      "   [0.36470588 0.52156863 0.72156863]\n",
      "   [0.37254902 0.52941176 0.72156863]\n",
      "   [0.33333333 0.50196078 0.70980392]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.27058824 0.54117647 0.7254902 ]\n",
      "   [0.03529412 0.36078431 0.54509804]\n",
      "   [0.19607843 0.44313725 0.62352941]\n",
      "   ...\n",
      "   [0.00784314 0.34117647 0.53333333]\n",
      "   [0.00392157 0.2627451  0.44705882]\n",
      "   [0.00784314 0.32156863 0.49803922]]\n",
      "\n",
      "  [[0.14117647 0.45882353 0.64705882]\n",
      "   [0.03921569 0.28235294 0.47843137]\n",
      "   [0.         0.23921569 0.38823529]\n",
      "   ...\n",
      "   [0.05490196 0.35294118 0.52156863]\n",
      "   [0.18823529 0.45882353 0.65490196]\n",
      "   [0.16470588 0.47843137 0.63137255]]\n",
      "\n",
      "  [[0.0745098  0.38431373 0.58039216]\n",
      "   [0.15686275 0.43137255 0.60784314]\n",
      "   [0.14509804 0.43529412 0.61568627]\n",
      "   ...\n",
      "   [0.00784314 0.39215686 0.5254902 ]\n",
      "   [0.02352941 0.30980392 0.49019608]\n",
      "   [0.02745098 0.36078431 0.5372549 ]]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 32, 32, 3)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_size = 32*32*3\n",
    "\n",
    "import numpy as np\n",
    "im = im.reshape(-1,32,32,3)/ 255\n",
    "print(im)\n",
    "im.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00688881, 0.00395561, 0.2065649 , 0.06371722, 0.6116261 ,\n",
       "        0.019401  , 0.05895606, 0.014032  , 0.00556926, 0.00928908]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = loaded_model.predict(im)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = r[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airplane = 0\n",
      "Automobile = 0\n",
      "Bird = 20\n",
      "Cat = 6\n",
      "Deer = 61\n",
      "Dog = 1\n",
      "Frog = 5\n",
      "Horse = 1\n",
      "Ship = 0\n",
      "Truck = 0\n",
      "Deer\n"
     ]
    }
   ],
   "source": [
    "for i, acc in enumerate(res) :\n",
    "    print(labels[i], \"=\", int(acc*100))\n",
    "print(labels[res.argmax()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code for image classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airplane = 0\n",
      "Automobile = 0\n",
      "Bird = 24\n",
      "Cat = 6\n",
      "Deer = 57\n",
      "Dog = 2\n",
      "Frog = 6\n",
      "Horse = 1\n",
      "Ship = 0\n",
      "Truck = 0\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "import urllib.request\n",
    "import os\n",
    "  \n",
    "# loading Python Imaging Library \n",
    "from PIL import ImageTk, Image \n",
    "  \n",
    "# To get the dialog box to open when required  \n",
    "#from tkinter import filedialog \n",
    "\n",
    "def img_processing():\n",
    "    from keras.models import model_from_json\n",
    "    json_file = open(\"model.json\", \"r\")\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "    loaded_model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])\n",
    "    \n",
    "    import cv2\n",
    "    im = cv2.imread('test.jpg') #사진 읽어들이기\n",
    "    \n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB) #색공간 변환\n",
    "    im = cv2.resize(im, (32,32)) #사이즈 \n",
    "    \n",
    "    in_size = 32*32*3\n",
    "\n",
    "    import numpy as np\n",
    "    im = im.reshape(-1,32,32,3)/ 255\n",
    "    \n",
    "    r = loaded_model.predict(im)\n",
    "    \n",
    "    res = r[0]\n",
    "    labels = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "    for i, acc in enumerate(res) :\n",
    "        print(labels[i], \"=\", int(acc*100))\n",
    "    return labels[res.argmax()]\n",
    "\n",
    "def download_img():\n",
    "    url = txt.get()\n",
    "    #print(url)\n",
    "    os.system(\"curl \" + url + \" > test.jpg\")\n",
    "    open_img()\n",
    "\n",
    "def open_img(): \n",
    "    # Select the Imagename  from a folder  \n",
    "    x = 'C:/Users/이석현/딥러닝/test.jpg' \n",
    "  \n",
    "    # opens the image \n",
    "    img = Image.open(x) \n",
    "      \n",
    "    # resize the image and apply a high-quality down sampling filter \n",
    "    img = img.resize((250, 250), Image.ANTIALIAS) \n",
    "  \n",
    "    # PhotoImage class is used to add image to widgets, icons etc \n",
    "    img = ImageTk.PhotoImage(img) \n",
    "   \n",
    "    # create a label \n",
    "    panel = Label(root, image = img) \n",
    "      \n",
    "    # set the image as img  \n",
    "    panel.image = img \n",
    "    panel.grid(row = 2)\n",
    "    \n",
    "    result = img_processing()\n",
    "    lbl3 = Label(root, text=result)\n",
    "    lbl3.grid(row=2,column=2)\n",
    "    \n",
    "# Create a windoe \n",
    "root = Tk() \n",
    "  \n",
    "# Set Title as Image Loader \n",
    "root.title(\"Image Classification\") \n",
    "  \n",
    "# Set the resolution of window \n",
    "root.geometry(\"550x300+300+150\") \n",
    "  \n",
    "# Allow Window to be resizable \n",
    "root.resizable(width = True, height = True) \n",
    "  \n",
    "# Create a button and place it into the window using grid layout \n",
    "txt = Entry(root, width=30)\n",
    "txt.grid(row=1)\n",
    "\n",
    "btn = Button(root, text ='Upload image', command = download_img).grid(row = 1, column=1, columnspan = 4) \n",
    "#print(txt.get())\n",
    "#open_img()\n",
    "#download_img(Entry.get(txt))\n",
    "lbl2 = Label(root, text=\"This image is: \")\n",
    "lbl2.grid(row=2,column=1)\n",
    "\n",
    "root.mainloop() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading Images in Tkinter Using PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/이석현/딥러닝/test.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in Tkinter callback\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Python\\Anaconda3\\lib\\tkinter\\__init__.py\", line 1705, in __call__\n",
      "    return self.func(*args)\n",
      "  File \"<ipython-input-71-036616a7400c>\", line 15, in open_img\n",
      "    img = Image.open(x)\n",
      "  File \"C:\\Python\\Anaconda3\\lib\\site-packages\\PIL\\Image.py\", line 2818, in open\n",
      "    raise IOError(\"cannot identify image file %r\" % (filename if filename else fp))\n",
      "OSError: cannot identify image file 'C:/Users/이석현/딥러닝/test.jpg'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/이석현/딥러닝/test1.jpg\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tkinter import *\n",
    "  \n",
    "# loading Python Imaging Library \n",
    "from PIL import ImageTk, Image \n",
    "  \n",
    "# To get the dialog box to open when required  \n",
    "from tkinter import filedialog \n",
    "\n",
    "def open_img(): \n",
    "    # Select the Imagename  from a folder  \n",
    "    x = openfilename() #C:/Users/이석현/딥러닝/test.jpg\n",
    "  \n",
    "    # opens the image \n",
    "    img = Image.open(x) \n",
    "      \n",
    "    # resize the image and apply a high-quality down sampling filter \n",
    "    img = img.resize((250, 250), Image.ANTIALIAS) \n",
    "  \n",
    "    # PhotoImage class is used to add image to widgets, icons etc \n",
    "    img = ImageTk.PhotoImage(img) \n",
    "   \n",
    "    # create a label \n",
    "    panel = Label(root, image = img) \n",
    "      \n",
    "    # set the image as img  \n",
    "    panel.image = img \n",
    "    panel.grid(row = 2) \n",
    "    \n",
    "def openfilename(): \n",
    "  \n",
    "    # open file dialog box to select image \n",
    "    # The dialogue box has a title \"Open\" \n",
    "    filename = filedialog.askopenfilename(title ='\"pen') \n",
    "    print(filename)\n",
    "    return filename \n",
    "\n",
    "# Create a windoe \n",
    "root = Tk() \n",
    "  \n",
    "# Set Title as Image Loader \n",
    "root.title(\"Image Loader\") \n",
    "  \n",
    "# Set the resolution of window \n",
    "root.geometry(\"550x300+300+150\") \n",
    "  \n",
    "# Allow Window to be resizable \n",
    "root.resizable(width = True, height = True) \n",
    "  \n",
    "# Create a button and place it into the window using grid layout \n",
    "btn = Button(root, text ='open image', command = open_img).grid( \n",
    "                                        row = 1, columnspan = 4) \n",
    "root.mainloop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Airplane = 0\n",
      "Automobile = 0\n",
      "Bird = 16\n",
      "Cat = 4\n",
      "Deer = 69\n",
      "Dog = 1\n",
      "Frog = 4\n",
      "Horse = 1\n",
      "Ship = 0\n",
      "Truck = 0\n"
     ]
    }
   ],
   "source": [
    "from tkinter import *\n",
    "import urllib.request\n",
    "import os\n",
    "  \n",
    "from PIL import ImageTk, Image \n",
    "\n",
    "#from tkinter import filedialog \n",
    "\n",
    "def img_processing():\n",
    "    from keras.models import model_from_json\n",
    "    json_file = open(\"model.json\", \"r\")\n",
    "    loaded_model_json = json_file.read()\n",
    "    json_file.close()\n",
    "    loaded_model = model_from_json(loaded_model_json)\n",
    "    loaded_model.load_weights(\"model.h5\")\n",
    "    loaded_model.compile(loss=\"binary_crossentropy\", optimizer=\"rmsprop\", metrics=['accuracy'])\n",
    "    \n",
    "    import cv2\n",
    "    im = cv2.imread('test.jpg')\n",
    "    \n",
    "    im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n",
    "    im = cv2.resize(im, (32,32)) \n",
    "    \n",
    "    in_size = 32*32*3\n",
    "\n",
    "    import numpy as np\n",
    "    im = im.reshape(-1,32,32,3)/ 255\n",
    "    \n",
    "    r = loaded_model.predict(im)\n",
    "    \n",
    "    res = r[0]\n",
    "    labels = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "    for i, acc in enumerate(res) :\n",
    "        print(labels[i], \"=\", int(acc*100))\n",
    "    return labels[res.argmax()]\n",
    "\n",
    "def download_img():\n",
    "    url = txt.get()\n",
    "    #print(url)\n",
    "    os.system(\"curl \" + url + \" > test.jpg\")\n",
    "    open_img()\n",
    "\n",
    "def open_img(): \n",
    "    x = 'C:/Users/이석현/딥러닝/test.jpg' \n",
    "    img = Image.open(x) \n",
    "    img = img.resize((250, 250), Image.ANTIALIAS) \n",
    "    img = ImageTk.PhotoImage(img) \n",
    "    panel = Label(root, image = img) \n",
    "    panel.image = img \n",
    "    panel.grid(row = 2)\n",
    "    \n",
    "    result = img_processing()\n",
    "    lbl3 = Label(root, text=result)\n",
    "    lbl3.grid(row=2,column=2)\n",
    "    \n",
    "root = Tk()  \n",
    "root.title(\"Image Classification\") \n",
    "root.geometry(\"550x300+300+150\") \n",
    "root.resizable(width = True, height = True) \n",
    "txt = Entry(root, width=30)\n",
    "txt.grid(row=1)\n",
    "\n",
    "btn = Button(root, text ='Upload image', command = download_img).grid(row = 1, column=1, columnspan = 4) \n",
    "lbl2 = Label(root, text=\"This image is: \")\n",
    "lbl2.grid(row=2,column=1)\n",
    "\n",
    "root.mainloop() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
